{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing required tools for analysis\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "from datetime import timedelta\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import f_oneway\n",
    "from s2sphere import CellId, LatLng\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style(\"white\", {'ytick.major.size': 10.0})\n",
    "sns.set_context(\"poster\", font_scale=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading the data and removing anomalies, duplicates etc. \n",
    "\"\"\"\n",
    "\n",
    "def get_data():\n",
    "    data = pd.read_csv(\"TH_data_challenge.tsv\",sep='\\t').drop_duplicates()\n",
    "    data = data[data['m_effective_daily_price'] > 0.0]\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_na_distribution():\n",
    "    data = get_data()\n",
    "    data_nan = (data.isnull().sum() / data.shape[0]) * 100\n",
    "    print data_nan\n",
    "\n",
    "def print_columns():\n",
    "    data = get_data()\n",
    "    columns = data.columns\n",
    "    print columns.values\n",
    "\n",
    "#print_columns()\n",
    "#get_na_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Feature Engineering\n",
    "'''\n",
    "\n",
    "def get_label_distribution():\n",
    "    data = get_data()\n",
    "    #label_data  = data['dim_is_requested']\n",
    "    #city_distribution = data['dim_market']\n",
    "    #print city_distribution.describe()\n",
    "    la_label = data[data['dim_market']=='Los Angeles']['dim_is_requested']\n",
    "    paris_label = data[data['dim_market']=='Paris']['dim_is_requested']\n",
    "    sf_label = data[data['dim_market']=='San Francisco']['dim_is_requested']\n",
    "    print la_label.describe()\n",
    "    print paris_label.describe()\n",
    "    print sf_label.describe()\n",
    "\n",
    "    \n",
    "def transform_date_to_day(date_col):\n",
    "    t_date = pd.DatetimeIndex(date_col)\n",
    "    t_date = t_date.weekday\n",
    "    return t_date\n",
    "\n",
    "def find_holiday_on_booking_date(date_col):\n",
    "    dr = pd.date_range(start='2015-01-01', end='2015-12-31')\n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "    t_date = pd.to_datetime(pd.Series(date_col))\n",
    "    df = pd.DataFrame()\n",
    "    df['date'] = t_date\n",
    "    df['is_holiday'] = df['date'].isin(holidays)\n",
    "    return df.is_holiday\n",
    "\n",
    "def find_nearby_holiday_on_booking_date(date_col):\n",
    "    dr = pd.date_range(start='2015-01-01', end='2015-12-31')\n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "    t_date = pd.to_datetime(pd.Series(date_col))\n",
    "    df = pd.DataFrame()\n",
    "    df['date'] = t_date\n",
    "    df['2_post_dates'] = df['date'] + timedelta(days=2)\n",
    "    df['1_post_date'] = df['date'] + timedelta(days=1)\n",
    "    df['2_pre_dates'] = df['date']  + timedelta(days=2)\n",
    "    df['1_pre_date'] = df['date']  + timedelta(days=1)\n",
    "    df['is_holiday_or_nearby'] = df['date'].isin(holidays) | df['2_post_dates'].isin(holidays) | df['1_post_date'].isin(holidays) | df['2_pre_dates'].isin(holidays) | df['1_pre_date'].isin(holidays)\n",
    "    return df.is_holiday_or_nearby\n",
    "\n",
    "def cluster_locality_and_value_data(data):\n",
    "    index = data.index\n",
    "    locality_data = data[['dim_lat', 'dim_lng', 'm_effective_daily_price', 'dim_person_capacity', 'm_total_overall_rating', 'm_reviews', 'm_professional_pictures']].fillna(0)\n",
    "    kmeans = KMeans(n_clusters=4)\n",
    "    locality_data = locality_data.values.tolist()\n",
    "    kmeans.fit(locality_data)\n",
    "    clusters = pd.Series(kmeans.predict(locality_data), index=index)\n",
    "    one_hot_clusters = pd.get_dummies(clusters, dummy_na = True)\n",
    "    one_hot_clusters.drop(np.nan,1,inplace=True)\n",
    "    return one_hot_clusters\n",
    "    \n",
    "    \n",
    "\n",
    "def s2_cell_id_spatial_data(lat_lng_data):\n",
    "    index = lat_lng_data.index\n",
    "    lat_data = lat_lng_data['dim_lat'].values.tolist()\n",
    "    lng_data = lat_lng_data['dim_lng'].values.tolist()\n",
    "    s2_data_cell_id = []\n",
    "    for lat,lng in zip(lat_data, lng_data):\n",
    "        cell_id = CellId.from_lat_lng(LatLng.from_degrees(lat, lng)).id()\n",
    "        s2_data_cell_id.append(cell_id)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    s2_data_cell_id = min_max_scaler.fit_transform(s2_data_cell_id).tolist()\n",
    "    s2_data_cell_id = pd.DataFrame(s2_data_cell_id, columns=['s2_cell_id'], index=index)\n",
    "    return s2_data_cell_id\n",
    "\n",
    "def price_and_night_feature(data):\n",
    "    index = data.index\n",
    "    list_price_la = data[data['dim_market'] == 'Los Angeles']['m_effective_daily_price']\n",
    "    list_price_p = data[data['dim_market'] == 'Paris']['m_effective_daily_price']\n",
    "    list_price_sf = data[data['dim_market'] == 'San Francisco']['m_effective_daily_price']\n",
    "    \n",
    "    list_price_la_norm = (list_price_la - list_price_la.mean()) / (list_price_la.max() - list_price_la.min())\n",
    "    list_price_p_norm = (list_price_p - list_price_p.mean()) / (list_price_p.max() - list_price_p.min())\n",
    "    list_price_sf_norm = (list_price_sf - list_price_sf.mean()) / (list_price_sf.max() - list_price_sf.min())\n",
    "    list_price = pd.concat([list_price_la_norm,list_price_p_norm,list_price_sf_norm])\n",
    "    \n",
    "    pn_feat = data[['m_effective_daily_price', 'm_pricing_cleaning_fee', 'price_booked_most_recent']]\n",
    "    #pn_feat.price_booked_most_recent.fillna(1.0029*pn_feat.m_effective_daily_price, inplace=True)\n",
    "    #pn_feat.price_booked_most_recent.fillna(1.05*pn_feat.m_effective_daily_price, inplace=True)\n",
    "    #pn_feat.price_booked_most_recent.fillna(pn_feat.price_booked_most_recent.mean(), inplace=True)\n",
    "    pn_feat.price_booked_most_recent.fillna(0, inplace=True)\n",
    "    \n",
    "    pn_feat['market_norm_price'] = list_price\n",
    "    \n",
    "    days_on_booking_night = transform_date_to_day(data['ds_night'])\n",
    "    curr_days = transform_date_to_day(data['ds'])\n",
    "    is_nearby_holiday = find_nearby_holiday_on_booking_date(data['ds_night']).astype(int)\n",
    "    is_holiday = find_holiday_on_booking_date(data['ds_night']).astype(int)\n",
    "    \n",
    "    is_nearby_holiday = pd.Series(is_nearby_holiday, index=index)\n",
    "    is_holiday = pd.Series(is_holiday, index=index)\n",
    "    curr_days = pd.Series(curr_days, index=index)\n",
    "    days_on_booking_night = pd.Series(days_on_booking_night, index=index)\n",
    "    \n",
    "    #one_hot_book_days = pd.get_dummies(days_on_booking_night)\n",
    "    #one_hot_book_days.columns = ['day_0', 'day_1', 'day_2', 'day_3', 'day_4', 'day_5', 'day_6']\n",
    "    #print days_on_booking_night.columns\n",
    "    #one_hot_book_days = one_hot_book_days.rename(columns={'0': 'day_0', '1': 'day_1','2': 'day_2','3': 'day_3','4': 'day_4','5': 'day_5','6': 'day_6' })\n",
    "    #one_hot_curr_days = pd.get_dummies(curr_days)\n",
    "    #one_hot_curr_days = curr_days.rename(columns={'0': 'curr_day_0', '1': 'curr_day_1','2': 'curr_day_2','3': 'curr_day_3','4': 'curr_day_4','5': 'curr_day_5','6': 'curr_day_6' })\n",
    "    #one_hot_book_days.columns = ['curr_day_0', 'curr_day_1', 'curr_day_2', 'curr_day_3', 'curr_day_4', 'curr_day_5', 'curr_day_6']\n",
    "    \n",
    "    days_on_booking_night = pd.DataFrame(days_on_booking_night,columns=['days_on_booking_night'])\n",
    "    curr_days = pd.DataFrame(curr_days,columns=['curr_days'])\n",
    "    is_nearby_holiday = pd.DataFrame(is_nearby_holiday,columns=['is_nearby_holiday'])\n",
    "    is_holiday = pd.DataFrame(is_holiday,columns=['is_holiday'])\n",
    "\n",
    "    pn_feat = pd.concat([pn_feat, days_on_booking_night, curr_days, is_holiday], axis=1)\n",
    "    \n",
    "    return pn_feat\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def listing_level_feature(data):\n",
    "    ls_feat = data[['dim_lat','dim_lng', 'dim_person_capacity','dim_is_instant_bookable','m_checkouts', 'm_reviews',\n",
    "              'days_since_last_booking','cancel_policy','image_quality_score', 'm_total_overall_rating','m_professional_pictures',\n",
    "              'dim_has_wireless_internet']]#.fillna(0)\n",
    "    \n",
    "    ls_feat = ls_feat.fillna(ls_feat.mean())\n",
    "    \n",
    "    room_type = pd.get_dummies(data['dim_room_type'])\n",
    "    lat_lng_s2_nromalized_cell = s2_cell_id_spatial_data(data[['dim_lat', 'dim_lng']])\n",
    "    \n",
    "    out = pd.concat([ls_feat, room_type], axis=1)\n",
    "    out = pd.concat([out, lat_lng_s2_nromalized_cell], axis=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "def occupancy_level_features(data):\n",
    "    oc_feat = data[['ds_night_day_of_week', 'ds_night_day_of_week', 'ds_checkin_gap', 'ds_checkout_gap', 'occ_occupancy_plus_minus_7_ds_night',\n",
    "                   'occ_occupancy_plus_minus_14_ds_night', 'occ_occupancy_trailing_90_ds', 'm_minimum_nights', 'm_maximum_nights']]#.fillna(0)\n",
    "    \n",
    "    oc_feat = oc_feat.apply(lambda x: x.fillna(x.median()),axis=0)\n",
    "    return oc_feat\n",
    "    \n",
    "\n",
    "def demand_level_features(data):\n",
    "    dl_feat = data[['price_booked_most_recent', 'p2_p3_click_through_score', 'p3_inquiry_score', 'listing_m_listing_views_2_6_ds_night_decay', \n",
    "                    'general_market_m_unique_searchers_0_6_ds_night', 'general_market_m_contacts_0_6_ds_night', \n",
    "                    'general_market_m_reservation_requests_0_6_ds_night', 'm_available_listings_ds_night']]#.fillna(0)\n",
    "    \n",
    "    #print dl_feat.describe()\n",
    "    #dl_feat = dl_feat.fillna(0)\n",
    "    dl_feat = dl_feat.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "    return dl_feat\n",
    "\n",
    "def kdt_features(data):\n",
    "    kdt_feat = data[['kdt_score', 'r_kdt_listing_views_0_6_avg_n100', 'r_kdt_n_active_n100', 'r_kdt_n_available_n100', \n",
    "                     'r_kdt_m_effective_daily_price_n100_p50', 'r_kdt_m_effective_daily_price_available_n100_p50',\n",
    "                    'r_kdt_m_effective_daily_price_booked_n100_p50']]\n",
    "    \n",
    "    #print kdt_feat.describe()\n",
    "    #kdt_feat = kdt_feat.fillna(0)\n",
    "    kdt_feat = kdt_feat.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "    one_hot_clusters = cluster_locality_and_value_data(data)\n",
    "    kdt_feat = pd.concat([kdt_feat, one_hot_clusters], axis=1)\n",
    "    return kdt_feat\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "MLP Modelling (One of the models out of all the ones being tested)\n",
    "'''\n",
    "\n",
    "def multi_layer_perceptron(X,Y):\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.array([Y, -(Y-1)]).T\n",
    "    X, X_test, Y, Y_test = train_test_split(X, Y, test_size=0.30)\n",
    "\n",
    "    # Parameters\n",
    "    learning_rate = 0.0001\n",
    "    training_epochs = 500\n",
    "    batch_size = 500\n",
    "    display_step = 1\n",
    "\n",
    "    # Network Parameters\n",
    "    n_hidden_1 = 70 # 1st layer number of features\n",
    "    n_hidden_2 = 70 # 2nd layer number of features\n",
    "    #n_hidden_3 = 50 # 2nd layer number of features\n",
    "    #n_hidden_4 = 20 # 2nd layer number of features\n",
    "    n_input = len(X[0]) # Number of feature\n",
    "    n_classes = 2 # Number of classes to predict\n",
    "\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "    # Create model\n",
    "    def multilayer_perceptron(x, weights, biases):\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "        # Hidden layer with RELU activation\n",
    "        #layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "        #layer_3 = tf.nn.relu(layer_3)\n",
    "        # Hidden layer with RELU activation\n",
    "        #layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "        #layer_4 = tf.nn.relu(layer_4)\n",
    "\n",
    "        # Output layer with linear activation\n",
    "        out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "        #out_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n",
    "        return out_layer\n",
    "\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        #'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "        #'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes])),\n",
    "        #'out': tf.Variable(tf.random_normal([n_hidden_4, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        #'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "        #'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    init = tf.initialize_all_variables()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(X)/batch_size)\n",
    "            X_batches = np.array_split(X, total_batch)\n",
    "            Y_batches = np.array_split(Y, total_batch)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = X_batches[i], Y_batches[i]\n",
    "                # batch_y.shape = (batch_y.shape[0], 1)\n",
    "                # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y})\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "            # Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                    \"{:.9f}\".format(avg_cost)\n",
    "        print \"Optimization Finished!\"\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print \"Accuracy:\", accuracy.eval({x: X_test, y: Y_test})\n",
    "        global result \n",
    "        result = tf.argmax(pred, 1).eval({x: X_test, y: Y_test})\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(sess.run(accuracy, feed_dict={x: X_test, y: Y_test}))\n",
    "\n",
    "        y_p = tf.argmax(pred, 1)\n",
    "        val_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={x:X_test, y:Y_test})\n",
    "\n",
    "        Y_test = [not i for i in Y_test[:,0]]    \n",
    "        y_pred =  y_pred.tolist()\n",
    "\n",
    "        printing_metrics(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ckhatri/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:324: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/ckhatri/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:359: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/ckhatri/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(184086, 54)\n",
      "(184086,)\n",
      "0.889863653648\n",
      "[ 0.90404258  0.85770084]\n",
      "[ 0.93511172  0.79758957]\n",
      "[ 0.91931472  0.82655374]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Model Building\n",
    "'''\n",
    "\n",
    "def get_label_and_features():\n",
    "    data = get_data()\n",
    "    #data = data[data['dim_market']=='Los Angeles']\n",
    "    #data = data[data['dim_market']=='Paris']\n",
    "    #data = data[data['dim_market']=='San Francisco']\n",
    "    market = data['dim_market']\n",
    "    market = pd.get_dummies(market)\n",
    "    \n",
    "    ls_feat = listing_level_feature(data)\n",
    "    os_feat = occupancy_level_features(data)\n",
    "    dl_feat = demand_level_features(data)\n",
    "    kdt_feat = kdt_features(data)\n",
    "    pn_feat = price_and_night_feature(data)\n",
    "    X = pd.concat([pn_feat, ls_feat, os_feat, dl_feat, kdt_feat], axis=1)\n",
    "    X = pd.concat([X,market], axis=1)\n",
    "    \n",
    "    #X_is_nan = (X.isnull().sum() / X.shape[0]) * 100\n",
    "    #print X_is_nan\n",
    "    features = X.columns\n",
    "    Y = data['dim_is_requested'].astype(int)\n",
    "    Y = Y.values#[:1000] \n",
    "    X = X.values#[:1000]\n",
    "    print X.shape\n",
    "    print Y.shape\n",
    "    return X,Y,features\n",
    "\n",
    "def printing_metrics(Y_test, Y_pred):\n",
    "    \n",
    "    (test_precision,test_recall,test_fscore,test_support)=precision_recall_fscore_support(Y_test, Y_pred, beta=1.0, labels=None,\n",
    "\t                                pos_label=1, average=None,\n",
    "\t                                warn_for=('precision', 'recall',\n",
    "\t                                          'f-score'),\n",
    "\t                                sample_weight=None)\n",
    "    \n",
    "    print accuracy_score(Y_test, Y_pred)\n",
    "    print test_precision\n",
    "    print test_recall\n",
    "    print test_fscore\n",
    "\n",
    "\n",
    "def gradient_boosting_model():\n",
    "    #cross validation and finding best params\n",
    "    cv_params = {'max_depth': [1,3,5,7,9,11], 'min_child_weight': [1,3,5]}\n",
    "    ind_params = {'learning_rate': 0.1, 'n_estimators': 1000, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "                 'objective': 'binary:logistic'}\n",
    "    \n",
    "    \n",
    "    #best params\n",
    "    cv_params = {'learning_rate': [0.1], 'subsample': [0.9]}\n",
    "    ind_params = {'n_estimators': 1000, 'seed':0, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'min_child_weight': 1, 'max_depth': 9}\n",
    "    \n",
    "    best_params = {'learning_rate': 0.1, 'seed':0, 'subsample': 0.9, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'max_depth':9, 'min_child_weight':1}\n",
    "\n",
    "    #model = GradientBoostingClassifier()\n",
    "    model = GridSearchCV(xgb.XGBClassifier(**ind_params), cv_params, scoring = 'accuracy', cv = 2, n_jobs = -1)\n",
    "    #model = xgb.XGBClassifier()\n",
    "    #model.set_params(**best_params)\n",
    "    \n",
    "    # After training GridSearch:\n",
    "    #print model.grid_scores_\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_feature_importance(model, features):\n",
    "    '''\n",
    "    %matplotlib inline\n",
    "    import seaborn as sns\n",
    "    sns.set(font_scale = 1.5)\n",
    "    xgb.plot_importance(model)\n",
    "    '''\n",
    "    # feature importance\n",
    "    feature_imp = model.feature_importances_\n",
    "    LABELS = range(len(feature_imp))\n",
    "    feat_imp_map = {}\n",
    "    \n",
    "    for feat,imp in zip(features, feature_imp):\n",
    "        #print feat, imp\n",
    "        feat_imp_map[feat] = imp\n",
    "        \n",
    "    sorted_feat_imp = sorted(feat_imp_map.items(), key=itemgetter(1), reverse=True)\n",
    "    top_feat = []\n",
    "    top_feat_val = []\n",
    "    for pair in sorted_feat_imp:\n",
    "        print pair[0], pair[1]\n",
    "        top_feat.append(pair[0])\n",
    "        top_feat_val.append(pair[1])\n",
    "    \n",
    "    top_feat = top_feat[:10]\n",
    "    top_feat_val = top_feat_val[:10]\n",
    "    \n",
    "    # plot\n",
    "    LABELS = range(len(top_feat_val))\n",
    "    pyplot.bar(LABELS, top_feat_val)\n",
    "    pyplot.xticks(LABELS, top_feat)\n",
    "    pyplot.show()\n",
    "\n",
    "    \n",
    "def build_classifier():\n",
    "    X,Y,features = get_label_and_features()\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    #multi_layer_perceptron(X,Y)\n",
    "    #model = LogisticRegression()\n",
    "    #model = SGDClassifier()\n",
    "    #model = SVC()\n",
    "    #model = LinearSVC()\n",
    "    #model = RandomForestClassifier()\n",
    "    model = gradient_boosting_model()\n",
    "    \n",
    "    model = model.fit(X_train,Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    printing_metrics(Y_test, Y_pred)\n",
    "    \n",
    "\n",
    "build_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
